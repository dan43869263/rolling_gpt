self.enc_in = 7
self.data_x = (8640, 7)
train 57463
self.enc_in = 7
self.data_x = (3216, 7)
val 19495
self.enc_in = 7
self.data_x = (3216, 7)
test 19495
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)













224it [00:28,  7.96it/s]
20it [00:01, 18.18it/s]

76it [00:04, 17.68it/s]
0it [00:00, ?it/s]
Epoch: 1, Steps: 224 | Train Loss: 0.4305135 Vali Loss: 0.2253586
lr = 0.0000993845













224it [00:27,  8.11it/s]
4it [00:00, 17.03it/s]


76it [00:04, 17.86it/s]
Epoch: 2, Steps: 224 | Train Loss: 0.3886173 Vali Loss: 0.2220492
lr = 0.0000975531
76it [00:04, 17.59it/s]













224it [00:27,  8.07it/s]
18it [00:01, 17.91it/s]


76it [00:04, 17.54it/s]
Epoch: 3, Steps: 224 | Train Loss: 0.3694169 Vali Loss: 0.2156641
lr = 0.0000945509
Validation loss decreased (0.222049 --> 0.215664).  Saving model ...













224it [00:27,  8.02it/s]
224it [00:27,  8.03it/s]

76it [00:04, 17.45it/s]
9it [00:01,  8.16it/s]
Epoch: 4, Steps: 224 | Train Loss: 0.3593468 Vali Loss: 0.2205036
lr = 0.0000904518













224it [00:27,  8.02it/s]
16it [00:00, 17.70it/s]

76it [00:04, 17.46it/s]
2it [00:00, 10.19it/s]
Epoch: 5, Steps: 224 | Train Loss: 0.3462865 Vali Loss: 0.2173103
lr = 0.0000853568













224it [00:27,  8.01it/s]
2it [00:00, 16.08it/s]


72it [00:04, 17.74it/s]
Epoch: 6, Steps: 224 | Train Loss: 0.3327033 Vali Loss: 0.2201491
lr = 0.0000793913
EarlyStopping counter: 3 out of 3
76it [00:04, 17.46it/s]
24it [00:01, 18.58it/s]


76it [00:04, 18.12it/s]
test shape: (76, 256, 96, 1) (76, 256, 96, 1)
test shape: (19456, 96, 1) (19456, 96, 1)
mae:0.3470, mse:0.2892, rmse:0.5378, r2:0.8125
mse_mean = 0.2892, mse_std = 0.0000
r2_mean = 0.8125, mae_std = 0.0000