train 761880
val 108675
test 219324
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)























































































































































































1003it [06:07,  2.73it/s]
	iters: 1000, epoch: 1 | loss: 0.3253796
























































































1488it [09:05,  2.73it/s]
2it [00:00,  5.92it/s]

















212it [00:34,  6.08it/s]
2it [00:00,  3.40it/s]
Epoch: 1, Steps: 1488 | Train Loss: 0.4366919 Vali Loss: 0.4003770
lr_adjust = {1: 0.0001}
Updating learning rate to 0.0001






















































































































































































996it [06:05,  2.73it/s]
	iters: 1000, epoch: 2 | loss: 0.2641073


























































































1487it [09:05,  2.72it/s]
1488it [09:05,  2.73it/s]
















203it [00:33,  6.10it/s]
Epoch: 2, Steps: 1488 | Train Loss: 0.4158704 Vali Loss: 0.3963420
lr_adjust = {2: 0.0001}
Updating learning rate to 0.0001

212it [00:34,  6.08it/s]






















































































































































































999it [06:06,  2.73it/s]
	iters: 1000, epoch: 3 | loss: 0.2922182

























































































1485it [09:04,  2.73it/s]
1488it [09:05,  2.73it/s]

















212it [00:34,  6.09it/s]
Epoch: 3, Steps: 1488 | Train Loss: 0.4064645 Vali Loss: 0.3926413
lr_adjust = {3: 0.0001}
Updating learning rate to 0.0001
Validation loss decreased (0.396342 --> 0.392641).  Saving model ...






















































































































































































997it [06:05,  2.73it/s]
	iters: 1000, epoch: 4 | loss: 0.2895369


























































































1487it [09:05,  2.73it/s]
1488it [09:05,  2.73it/s]
















204it [00:33,  6.11it/s]
Epoch: 4, Steps: 1488 | Train Loss: 0.4004440 Vali Loss: 0.3927396
lr_adjust = {4: 9e-05}
Updating learning rate to 9e-05
212it [00:34,  6.08it/s]






















































































































































































995it [06:04,  2.73it/s]
	iters: 1000, epoch: 5 | loss: 0.2956237


























































































1487it [09:05,  2.72it/s]
1488it [09:05,  2.73it/s]
















204it [00:33,  6.11it/s]
Epoch: 5, Steps: 1488 | Train Loss: 0.3952780 Vali Loss: 0.3918838
lr_adjust = {5: 8.1e-05}
Updating learning rate to 8.1e-05

212it [00:34,  6.08it/s]






















































































































































































999it [06:06,  2.72it/s]
	iters: 1000, epoch: 6 | loss: 0.4451347

























































































1485it [09:04,  2.73it/s]
1488it [09:05,  2.73it/s]

















208it [00:34,  6.12it/s]
Epoch: 6, Steps: 1488 | Train Loss: 0.3916409 Vali Loss: 0.3901890
lr_adjust = {6: 7.290000000000001e-05}
Updating learning rate to 7.290000000000001e-05
212it [00:34,  6.08it/s]























































































































































































1001it [06:06,  2.73it/s]
	iters: 1000, epoch: 7 | loss: 0.2384269

























































































1487it [09:05,  2.73it/s]
1488it [09:05,  2.73it/s]
















205it [00:33,  6.10it/s]
Epoch: 7, Steps: 1488 | Train Loss: 0.3885068 Vali Loss: 0.3949902
lr_adjust = {7: 6.561e-05}
Updating learning rate to 6.561e-05
212it [00:34,  6.09it/s]























































































































































































1000it [06:06,  2.72it/s]
	iters: 1000, epoch: 8 | loss: 0.3508808

























































































1486it [09:04,  2.72it/s]
1488it [09:05,  2.73it/s]
















212it [00:34,  6.08it/s]
0it [00:00, ?it/s]
Epoch: 8, Steps: 1488 | Train Loss: 0.3859764 Vali Loss: 0.3937801
lr_adjust = {8: 5.904900000000001e-05}
Updating learning rate to 5.904900000000001e-05























































































































































































1000it [06:06,  2.72it/s]
	iters: 1000, epoch: 9 | loss: 0.2482955

























































































1486it [09:04,  2.73it/s]
1488it [09:05,  2.73it/s]
















212it [00:34,  6.08it/s]
0it [00:00, ?it/s]
Epoch: 9, Steps: 1488 | Train Loss: 0.3839215 Vali Loss: 0.3963178
lr_adjust = {9: 5.3144100000000005e-05}
Updating learning rate to 5.3144100000000005e-05
EarlyStopping counter: 3 out of 3
Early stopping


































428it [01:06,  6.39it/s]
test shape: (428, 512, 96, 1) (428, 512, 96, 1)
test shape: (219136, 96, 1) (219136, 96, 1)
mae:0.2011, mse:0.1496, rmse:0.3868, r2:0.7403
train 761880
val 108675
test 219324
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)






















































































































































































997it [06:05,  2.73it/s]
	iters: 1000, epoch: 1 | loss: 0.2618438


























































































1488it [09:05,  2.73it/s]
Epoch: 1 cost time: 546.0304749011993
















206it [00:33,  6.09it/s]
Epoch: 1, Steps: 1488 | Train Loss: 0.4412300 Vali Loss: 0.4079865
lr_adjust = {1: 0.0001}
Updating learning rate to 0.0001
212it [00:34,  6.07it/s]























































































































































































1000it [06:06,  2.73it/s]
	iters: 1000, epoch: 2 | loss: 0.5399136

























































































1485it [09:04,  2.73it/s]
1488it [09:05,  2.73it/s]

















211it [00:34,  6.08it/s]
Epoch: 2, Steps: 1488 | Train Loss: 0.4200423 Vali Loss: 0.3985680
lr_adjust = {2: 0.0001}
Updating learning rate to 0.0001
212it [00:34,  6.08it/s]























































































































































































1002it [06:07,  2.73it/s]
	iters: 1000, epoch: 3 | loss: 0.2546390

























































































1488it [09:05,  2.73it/s]
Epoch: 3 cost time: 546.0775237083435
















212it [00:34,  6.08it/s]
0it [00:00, ?it/s]
Epoch: 3, Steps: 1488 | Train Loss: 0.4104376 Vali Loss: 0.3938985
lr_adjust = {3: 0.0001}
Updating learning rate to 0.0001























































































































































































998it [06:05,  2.72it/s]
	iters: 1000, epoch: 4 | loss: 0.3687000

























































































1488it [09:05,  2.73it/s]
2it [00:00,  5.86it/s]

















210it [00:34,  6.12it/s]
Epoch: 4, Steps: 1488 | Train Loss: 0.4034571 Vali Loss: 0.3933532
lr_adjust = {4: 9e-05}
Updating learning rate to 9e-05
212it [00:34,  6.09it/s]























































































































































































1001it [06:07,  2.73it/s]
	iters: 1000, epoch: 5 | loss: 1.0623387

























































































1487it [09:05,  2.73it/s]
1488it [09:05,  2.73it/s]
















212it [00:34,  6.08it/s]
1it [00:00,  5.36it/s]
Epoch: 5, Steps: 1488 | Train Loss: 0.3984043 Vali Loss: 0.3943093
lr_adjust = {5: 8.1e-05}
Updating learning rate to 8.1e-05























































































































































































999it [06:06,  2.73it/s]
	iters: 1000, epoch: 6 | loss: 0.9471902

























































































1488it [09:05,  2.73it/s]
5it [00:00,  5.77it/s]

















212it [00:34,  6.09it/s]
Epoch: 6, Steps: 1488 | Train Loss: 0.3948189 Vali Loss: 0.3954693
lr_adjust = {6: 7.290000000000001e-05}
Updating learning rate to 7.290000000000001e-05
EarlyStopping counter: 2 out of 3






















































































































































































999it [06:06,  2.72it/s]
	iters: 1000, epoch: 7 | loss: 0.3764734

























































































1488it [09:05,  2.73it/s]
2it [00:00,  5.79it/s]

















209it [00:34,  6.09it/s]
Epoch: 7, Steps: 1488 | Train Loss: 0.3907589 Vali Loss: 0.3932614
lr_adjust = {7: 6.561e-05}
Updating learning rate to 6.561e-05
212it [00:34,  6.07it/s]























































































































































































1001it [06:07,  2.73it/s]
	iters: 1000, epoch: 8 | loss: 0.2054789

























































































1487it [09:05,  2.73it/s]
1488it [09:05,  2.73it/s]
















212it [00:34,  6.09it/s]
1it [00:00,  4.38it/s]
Epoch: 8, Steps: 1488 | Train Loss: 0.3885577 Vali Loss: 0.3968309
lr_adjust = {8: 5.904900000000001e-05}
Updating learning rate to 5.904900000000001e-05























































































































































































999it [06:06,  2.73it/s]
	iters: 1000, epoch: 9 | loss: 0.2311968

























































































1488it [09:05,  2.73it/s]
5it [00:00,  5.87it/s]

















212it [00:34,  6.09it/s]
Epoch: 9, Steps: 1488 | Train Loss: 0.3859982 Vali Loss: 0.3974281
lr_adjust = {9: 5.3144100000000005e-05}
Updating learning rate to 5.3144100000000005e-05
EarlyStopping counter: 2 out of 3






















































































































































































999it [06:06,  2.73it/s]
	iters: 1000, epoch: 10 | loss: 0.4583826

























































































1488it [09:05,  2.73it/s]
5it [00:00,  5.73it/s]

















212it [00:34,  6.07it/s]
Epoch: 10, Steps: 1488 | Train Loss: 0.3842214 Vali Loss: 0.4028278
lr_adjust = {10: 4.782969000000001e-05}
Updating learning rate to 4.782969000000001e-05
EarlyStopping counter: 3 out of 3
Early stopping
10it [00:01,  6.37it/s]

































427it [01:07,  6.36it/s]
test shape: (428, 512, 96, 1) (428, 512, 96, 1)
428it [01:07,  6.35it/s]
1it [00:00,  5.02it/s]
mae:0.2008, mse:0.1486, rmse:0.3855, r2:0.7420
train 761880
val 108675
test 219324
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)






















































































































































































995it [06:04,  2.73it/s]
	iters: 1000, epoch: 1 | loss: 0.8341755


























































































1487it [09:05,  2.73it/s]
1488it [09:05,  2.73it/s]
















204it [00:33,  6.11it/s]
Epoch: 1, Steps: 1488 | Train Loss: 0.4456767 Vali Loss: 0.4075594
lr_adjust = {1: 0.0001}
Updating learning rate to 0.0001

212it [00:34,  6.08it/s]






















































































































































































998it [06:05,  2.72it/s]
	iters: 1000, epoch: 2 | loss: 0.3342636

























































































1484it [09:04,  2.73it/s]
1488it [09:05,  2.73it/s]

















209it [00:34,  6.10it/s]
Epoch: 2, Steps: 1488 | Train Loss: 0.4218551 Vali Loss: 0.3994331
lr_adjust = {2: 0.0001}
Updating learning rate to 0.0001
212it [00:34,  6.09it/s]






















































































































































































995it [06:04,  2.72it/s]
	iters: 1000, epoch: 3 | loss: 0.3167574


























































































1487it [09:05,  2.73it/s]
1488it [09:05,  2.73it/s]

















212it [00:34,  6.09it/s]
Epoch: 3, Steps: 1488 | Train Loss: 0.4119209 Vali Loss: 0.3960052
lr_adjust = {3: 0.0001}
Updating learning rate to 0.0001
Validation loss decreased (0.399433 --> 0.396005).  Saving model ...






















































































































































































997it [06:05,  2.73it/s]
	iters: 1000, epoch: 4 | loss: 0.3410092

























































































1488it [09:05,  2.73it/s]
0it [00:00, ?it/s]

















207it [00:33,  6.09it/s]
Epoch: 4, Steps: 1488 | Train Loss: 0.4050160 Vali Loss: 0.3981233
lr_adjust = {4: 9e-05}
Updating learning rate to 9e-05
212it [00:34,  6.08it/s]






















































































































































































996it [06:05,  2.73it/s]
	iters: 1000, epoch: 5 | loss: 0.2967497


























































































1488it [09:05,  2.73it/s]
Epoch: 5 cost time: 546.1010847091675
















212it [00:34,  6.08it/s]
0it [00:00, ?it/s]
Epoch: 5, Steps: 1488 | Train Loss: 0.3992262 Vali Loss: 0.3983853
lr_adjust = {5: 8.1e-05}
Updating learning rate to 8.1e-05























































































































































































1000it [06:06,  2.72it/s]
	iters: 1000, epoch: 6 | loss: 0.2511961

























































































1486it [09:04,  2.73it/s]
1488it [09:05,  2.73it/s]

















212it [00:34,  6.07it/s]
Epoch: 6, Steps: 1488 | Train Loss: 0.3949936 Vali Loss: 0.3976777
lr_adjust = {6: 7.290000000000001e-05}
Updating learning rate to 7.290000000000001e-05
EarlyStopping counter: 3 out of 3
Early stopping
------------------------------------

































428it [01:07,  6.36it/s]
test shape: (428, 512, 96, 1) (428, 512, 96, 1)
test shape: (219136, 96, 1) (219136, 96, 1)
mae:0.2012, mse:0.1500, rmse:0.3873, r2:0.7397
mse_mean = 0.1494, mse_std = 0.0006
r2_mean = 0.7397, mae_std = 0.0000