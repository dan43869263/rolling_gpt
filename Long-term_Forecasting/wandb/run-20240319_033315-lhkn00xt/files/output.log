train 759864
val 106659
test 217308
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)























































































































































































998it [06:07,  2.72it/s]
	iters: 1000, epoch: 1 | loss: 0.4082229

























































































1483it [09:06,  2.72it/s]
1484it [09:06,  2.71it/s]
















203it [00:33,  6.07it/s]
Epoch: 1, Steps: 1484 | Train Loss: 0.4906787 Vali Loss: 0.4773728
lr_adjust = {1: 0.0001}
Updating learning rate to 0.0001
208it [00:34,  6.05it/s]























































































































































































998it [06:06,  2.72it/s]
	iters: 1000, epoch: 2 | loss: 0.4522879

























































































1482it [09:04,  2.73it/s]
1484it [09:05,  2.72it/s]
















201it [00:33,  6.08it/s]
Epoch: 2, Steps: 1484 | Train Loss: 0.4731237 Vali Loss: 0.4652546
lr_adjust = {2: 0.0001}
Updating learning rate to 0.0001

208it [00:34,  6.06it/s]






















































































































































































997it [06:06,  2.72it/s]
	iters: 1000, epoch: 3 | loss: 0.5279672

























































































1482it [09:04,  2.72it/s]
1484it [09:05,  2.72it/s]
















201it [00:33,  6.07it/s]
Epoch: 3, Steps: 1484 | Train Loss: 0.4631070 Vali Loss: 0.4606159
lr_adjust = {3: 0.0001}
Updating learning rate to 0.0001

208it [00:34,  6.06it/s]






















































































































































































997it [06:06,  2.72it/s]
	iters: 1000, epoch: 4 | loss: 0.5570712

























































































1480it [09:03,  2.72it/s]
1484it [09:05,  2.72it/s]

















208it [00:34,  6.06it/s]
Epoch: 4, Steps: 1484 | Train Loss: 0.4552667 Vali Loss: 0.4635741
lr_adjust = {4: 9e-05}
Updating learning rate to 9e-05
EarlyStopping counter: 1 out of 3






















































































































































































997it [06:06,  2.72it/s]
	iters: 1000, epoch: 5 | loss: 0.3138525

























































































1482it [09:04,  2.72it/s]
1484it [09:05,  2.72it/s]
















201it [00:33,  6.08it/s]
Epoch: 5, Steps: 1484 | Train Loss: 0.4495634 Vali Loss: 0.4643987
lr_adjust = {5: 8.1e-05}
Updating learning rate to 8.1e-05
208it [00:34,  6.06it/s]























































































































































































998it [06:06,  2.73it/s]
	iters: 1000, epoch: 6 | loss: 0.7194383

























































































1483it [09:04,  2.72it/s]
1484it [09:05,  2.72it/s]
















202it [00:33,  6.10it/s]
Epoch: 6, Steps: 1484 | Train Loss: 0.4448770 Vali Loss: 0.4652819
lr_adjust = {6: 7.290000000000001e-05}
Updating learning rate to 7.290000000000001e-05
EarlyStopping counter: 3 out of 3
Early stopping
208it [00:34,  6.06it/s]

































424it [01:06,  6.37it/s]
test shape: (424, 512, 192, 1) (424, 512, 192, 1)
test shape: (217088, 192, 1) (217088, 192, 1)
mae:0.2439, mse:0.1965, rmse:0.4433, r2:0.6598
train 759864
val 106659
test 217308
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)























































































































































































1000it [06:07,  2.72it/s]
	iters: 1000, epoch: 1 | loss: 0.6606821

























































































1483it [09:05,  2.72it/s]
1484it [09:05,  2.72it/s]
















204it [00:33,  6.09it/s]
Epoch: 1, Steps: 1484 | Train Loss: 0.4939802 Vali Loss: 0.4782979
lr_adjust = {1: 0.0001}
Updating learning rate to 0.0001
208it [00:34,  6.06it/s]























































































































































































998it [06:06,  2.72it/s]
	iters: 1000, epoch: 2 | loss: 0.8711232

























































































1483it [09:05,  2.72it/s]
1484it [09:05,  2.72it/s]
















202it [00:33,  6.08it/s]
Epoch: 2, Steps: 1484 | Train Loss: 0.4747035 Vali Loss: 0.4672066
lr_adjust = {2: 0.0001}
Updating learning rate to 0.0001
208it [00:34,  6.05it/s]
























































































































































































1002it [06:08,  2.72it/s]
	iters: 1000, epoch: 3 | loss: 0.3684877
























































































1484it [09:05,  2.72it/s]
2it [00:00,  5.92it/s]

















208it [00:34,  6.06it/s]
Epoch: 3, Steps: 1484 | Train Loss: 0.4640840 Vali Loss: 0.4645787
lr_adjust = {3: 0.0001}
Updating learning rate to 0.0001
Validation loss decreased (0.467207 --> 0.464579).  Saving model ...























































































































































































999it [06:07,  2.72it/s]
	iters: 1000, epoch: 4 | loss: 0.6131418

























































































1484it [09:05,  2.72it/s]
Epoch: 4 cost time: 546.2169146537781
















205it [00:33,  6.08it/s]
Epoch: 4, Steps: 1484 | Train Loss: 0.4568950 Vali Loss: 0.4593944
lr_adjust = {4: 9e-05}
Updating learning rate to 9e-05
208it [00:34,  6.05it/s]























































































































































































997it [06:06,  2.72it/s]
	iters: 1000, epoch: 5 | loss: 0.3420559

























































































1482it [09:04,  2.72it/s]
1484it [09:05,  2.72it/s]
















208it [00:34,  6.06it/s]
1it [00:00,  4.59it/s]
Epoch: 5, Steps: 1484 | Train Loss: 0.4505980 Vali Loss: 0.4615506
lr_adjust = {5: 8.1e-05}
Updating learning rate to 8.1e-05























































































































































































998it [06:06,  2.72it/s]
	iters: 1000, epoch: 6 | loss: 0.7673608

























































































1482it [09:04,  2.72it/s]
1484it [09:05,  2.72it/s]
















208it [00:34,  6.06it/s]
2it [00:00,  3.38it/s]
Epoch: 6, Steps: 1484 | Train Loss: 0.4461120 Vali Loss: 0.4600918
lr_adjust = {6: 7.290000000000001e-05}
Updating learning rate to 7.290000000000001e-05























































































































































































997it [06:06,  2.72it/s]
	iters: 1000, epoch: 7 | loss: 0.2827588

























































































1482it [09:04,  2.72it/s]
1484it [09:05,  2.72it/s]
















208it [00:34,  6.06it/s]
1it [00:00,  5.81it/s]
Epoch: 7, Steps: 1484 | Train Loss: 0.4419295 Vali Loss: 0.4621968
lr_adjust = {7: 6.561e-05}
Updating learning rate to 6.561e-05
EarlyStopping counter: 3 out of 3
Early stopping

































420it [01:06,  6.40it/s]
test shape: (424, 512, 192, 1) (424, 512, 192, 1)

424it [01:06,  6.33it/s]
mae:0.2435, mse:0.1945, rmse:0.4410, r2:0.6633
train 759864
val 106659
test 217308
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)























































































































































































1001it [06:07,  2.72it/s]
	iters: 1000, epoch: 1 | loss: 0.3794026

























































































1484it [09:05,  2.72it/s]
Epoch: 1 cost time: 545.914178609848
















206it [00:33,  6.10it/s]
Epoch: 1, Steps: 1484 | Train Loss: 0.5023462 Vali Loss: 0.4892087
lr_adjust = {1: 0.0001}
Updating learning rate to 0.0001
208it [00:34,  6.07it/s]























































































































































































999it [06:07,  2.72it/s]
	iters: 1000, epoch: 2 | loss: 0.3301915

























































































1484it [09:05,  2.72it/s]
Epoch: 2 cost time: 545.7572109699249
















206it [00:33,  6.09it/s]
Epoch: 2, Steps: 1484 | Train Loss: 0.4791137 Vali Loss: 0.4736499
lr_adjust = {2: 0.0001}
Updating learning rate to 0.0001
208it [00:34,  6.06it/s]























































































































































































998it [06:06,  2.72it/s]
	iters: 1000, epoch: 3 | loss: 0.3459637

























































































1483it [09:04,  2.72it/s]
1484it [09:05,  2.72it/s]
















208it [00:34,  6.07it/s]
0it [00:00, ?it/s]
Epoch: 3, Steps: 1484 | Train Loss: 0.4700599 Vali Loss: 0.4634428
lr_adjust = {3: 0.0001}
Updating learning rate to 0.0001
























































































































































































1002it [06:08,  2.72it/s]
	iters: 1000, epoch: 4 | loss: 0.3066556
























































































1484it [09:05,  2.72it/s]
5it [00:00,  5.75it/s]

















208it [00:34,  6.06it/s]
Epoch: 4, Steps: 1484 | Train Loss: 0.4621484 Vali Loss: 0.4605393
lr_adjust = {4: 9e-05}
Updating learning rate to 9e-05
Validation loss decreased (0.463443 --> 0.460539).  Saving model ...























































































































































































1000it [06:07,  2.72it/s]
	iters: 1000, epoch: 5 | loss: 0.4106068
























































































1484it [09:05,  2.72it/s]
0it [00:00, ?it/s]

















208it [00:34,  6.11it/s]
Epoch: 5, Steps: 1484 | Train Loss: 0.4556305 Vali Loss: 0.4634593
lr_adjust = {5: 8.1e-05}
Updating learning rate to 8.1e-05
208it [00:34,  6.07it/s]























































































































































































1001it [06:07,  2.72it/s]
	iters: 1000, epoch: 6 | loss: 0.3610536

























































































1484it [09:05,  2.72it/s]
Epoch: 6 cost time: 546.0775117874146















194it [00:31,  6.09it/s]
Epoch: 6, Steps: 1484 | Train Loss: 0.4508239 Vali Loss: 0.4633690
lr_adjust = {6: 7.290000000000001e-05}
Updating learning rate to 7.290000000000001e-05

208it [00:34,  6.06it/s]






















































































































































































995it [06:05,  2.72it/s]
	iters: 1000, epoch: 7 | loss: 0.3916152

























































































1480it [09:04,  2.72it/s]
1484it [09:05,  2.72it/s]

















207it [00:34,  6.07it/s]
Epoch: 7, Steps: 1484 | Train Loss: 0.4469796 Vali Loss: 0.4643825
lr_adjust = {7: 6.561e-05}
Updating learning rate to 6.561e-05
EarlyStopping counter: 3 out of 3
Early stopping
208it [00:34,  6.05it/s]
































413it [01:05,  6.32it/s]
test shape: (424, 512, 192, 1) (424, 512, 192, 1)

424it [01:07,  6.31it/s]
mae:0.2426, mse:0.1952, rmse:0.4418, r2:0.6621
mse_mean = 0.1954, mse_std = 0.0009
r2_mean = 0.6617, mae_std = 0.0015