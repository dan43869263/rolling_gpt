train 756840
val 103635
test 214284
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
























































































































































































1000it [06:10,  2.71it/s]
	iters: 1000, epoch: 1 | loss: 0.3860311
























































































1478it [09:06,  2.70it/s]
Epoch: 1 cost time: 546.7694668769836















192it [00:31,  6.09it/s]
Epoch: 1, Steps: 1478 | Train Loss: 0.5405924 Vali Loss: 0.5522535
lr_adjust = {1: 0.0001}
Updating learning rate to 0.0001

202it [00:33,  6.05it/s]























































































































































































997it [06:07,  2.71it/s]
	iters: 1000, epoch: 2 | loss: 0.8046058
























































































1475it [09:04,  2.71it/s]
1478it [09:05,  2.71it/s]
















194it [00:32,  6.08it/s]
Epoch: 2, Steps: 1478 | Train Loss: 0.5251977 Vali Loss: 0.5400072
lr_adjust = {2: 0.0001}
Updating learning rate to 0.0001

202it [00:33,  6.05it/s]























































































































































































997it [06:07,  2.71it/s]
	iters: 1000, epoch: 3 | loss: 0.4020205
























































































1475it [09:04,  2.71it/s]
1478it [09:05,  2.71it/s]
















198it [00:32,  6.09it/s]
Epoch: 3, Steps: 1478 | Train Loss: 0.5157641 Vali Loss: 0.5362169
lr_adjust = {3: 0.0001}
Updating learning rate to 0.0001
202it [00:33,  6.05it/s]
























































































































































































999it [06:08,  2.71it/s]
	iters: 1000, epoch: 4 | loss: 0.4504005
























































































1477it [09:04,  2.71it/s]
1478it [09:05,  2.71it/s]
















202it [00:33,  6.05it/s]
Epoch: 4, Steps: 1478 | Train Loss: 0.5087545 Vali Loss: 0.5324906
lr_adjust = {4: 9e-05}
Updating learning rate to 9e-05
Validation loss decreased (0.536217 --> 0.532491).  Saving model ...
























































































































































































1000it [06:08,  2.71it/s]
	iters: 1000, epoch: 5 | loss: 0.3959392
























































































1478it [09:05,  2.71it/s]
Epoch: 5 cost time: 545.7393760681152
















202it [00:33,  6.04it/s]
Epoch: 5, Steps: 1478 | Train Loss: 0.5029678 Vali Loss: 0.5355149
lr_adjust = {5: 8.1e-05}
Updating learning rate to 8.1e-05
EarlyStopping counter: 1 out of 3























































































































































































999it [06:08,  2.72it/s]
	iters: 1000, epoch: 6 | loss: 0.3502528
























































































1476it [09:04,  2.71it/s]
1478it [09:05,  2.71it/s]
















198it [00:32,  6.08it/s]
Epoch: 6, Steps: 1478 | Train Loss: 0.4977109 Vali Loss: 0.5357748
lr_adjust = {6: 7.290000000000001e-05}
Updating learning rate to 7.290000000000001e-05
202it [00:33,  6.05it/s]























































































































































































996it [06:07,  2.71it/s]
	iters: 1000, epoch: 7 | loss: 0.5404460
























































































1474it [09:03,  2.71it/s]
1478it [09:05,  2.71it/s]
















196it [00:32,  6.07it/s]
Epoch: 7, Steps: 1478 | Train Loss: 0.4933665 Vali Loss: 0.5377787
lr_adjust = {7: 6.561e-05}
Updating learning rate to 6.561e-05
EarlyStopping counter: 3 out of 3
Early stopping
202it [00:33,  6.05it/s]
































411it [01:04,  6.37it/s]
test shape: (418, 512, 336, 1) (418, 512, 336, 1)

418it [01:05,  6.36it/s]
mae:0.2857, mse:0.2495, rmse:0.4995, r2:0.5701
train 756840
val 103635
test 214284
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)























































































































































































996it [06:07,  2.71it/s]
	iters: 1000, epoch: 1 | loss: 0.5059884
























































































1478it [09:05,  2.71it/s]
0it [00:00, ?it/s]
















195it [00:32,  6.06it/s]
Epoch: 1, Steps: 1478 | Train Loss: 0.5442067 Vali Loss: 0.5517510
lr_adjust = {1: 0.0001}
Updating learning rate to 0.0001

202it [00:33,  6.04it/s]























































































































































































998it [06:08,  2.72it/s]
	iters: 1000, epoch: 2 | loss: 0.4943736
























































































1476it [09:04,  2.71it/s]
1478it [09:05,  2.71it/s]
















200it [00:33,  6.07it/s]
Epoch: 2, Steps: 1478 | Train Loss: 0.5271401 Vali Loss: 0.5421201
lr_adjust = {2: 0.0001}
Updating learning rate to 0.0001
202it [00:33,  6.04it/s]
























































































































































































999it [06:08,  2.70it/s]
	iters: 1000, epoch: 3 | loss: 0.4384088
























































































1476it [09:04,  2.70it/s]
1478it [09:05,  2.71it/s]
















201it [00:33,  6.06it/s]
Epoch: 3, Steps: 1478 | Train Loss: 0.5184557 Vali Loss: 0.5377793
lr_adjust = {3: 0.0001}
Updating learning rate to 0.0001
202it [00:33,  6.04it/s]
























































































































































































1001it [06:09,  2.71it/s]
	iters: 1000, epoch: 4 | loss: 0.5531594
























































































1478it [09:05,  2.71it/s]
Epoch: 4 cost time: 545.7598402500153
















202it [00:33,  6.05it/s]
Epoch: 4, Steps: 1478 | Train Loss: 0.5108812 Vali Loss: 0.5362693
lr_adjust = {4: 9e-05}
Updating learning rate to 9e-05
Validation loss decreased (0.537779 --> 0.536269).  Saving model ...
























































































































































































1001it [06:09,  2.71it/s]
	iters: 1000, epoch: 5 | loss: 0.5190853























































































1478it [09:05,  2.71it/s]
0it [00:00, ?it/s]
















202it [00:33,  6.05it/s]
1it [00:00,  5.23it/s]
Epoch: 5, Steps: 1478 | Train Loss: 0.5044023 Vali Loss: 0.5374258
lr_adjust = {5: 8.1e-05}
Updating learning rate to 8.1e-05
























































































































































































1000it [06:08,  2.71it/s]
	iters: 1000, epoch: 6 | loss: 0.4352728
























































































1478it [09:05,  2.71it/s]
Epoch: 6 cost time: 545.6124649047852
















202it [00:33,  6.05it/s]
Epoch: 6, Steps: 1478 | Train Loss: 0.4992781 Vali Loss: 0.5355285
lr_adjust = {6: 7.290000000000001e-05}
Updating learning rate to 7.290000000000001e-05
Validation loss decreased (0.536269 --> 0.535528).  Saving model ...
























































































































































































1001it [06:09,  2.71it/s]
	iters: 1000, epoch: 7 | loss: 0.4630329
























































































1478it [09:05,  2.71it/s]
Epoch: 7 cost time: 545.8359544277191















202it [00:33,  6.05it/s]
0it [00:00, ?it/s]
Epoch: 7, Steps: 1478 | Train Loss: 0.4946294 Vali Loss: 0.5371813
lr_adjust = {7: 6.561e-05}
Updating learning rate to 6.561e-05
























































































































































































999it [06:08,  2.71it/s]
	iters: 1000, epoch: 8 | loss: 0.7382771
























































































1476it [09:04,  2.71it/s]
1478it [09:05,  2.71it/s]
















199it [00:32,  6.06it/s]
Epoch: 8, Steps: 1478 | Train Loss: 0.4909505 Vali Loss: 0.5394473
lr_adjust = {8: 5.904900000000001e-05}
Updating learning rate to 5.904900000000001e-05
202it [00:33,  6.05it/s]
























































































































































































1002it [06:09,  2.71it/s]
	iters: 1000, epoch: 9 | loss: 0.3392294























































































1478it [09:05,  2.71it/s]
3it [00:00,  5.91it/s]
















202it [00:33,  6.04it/s]
4it [00:00,  6.25it/s]
Epoch: 9, Steps: 1478 | Train Loss: 0.4882367 Vali Loss: 0.5398439
lr_adjust = {9: 5.3144100000000005e-05}
Updating learning rate to 5.3144100000000005e-05
EarlyStopping counter: 3 out of 3
Early stopping

































418it [01:06,  6.31it/s]
test shape: (418, 512, 336, 1) (418, 512, 336, 1)
test shape: (214016, 336, 1) (214016, 336, 1)
mae:0.2916, mse:0.2575, rmse:0.5074, r2:0.5563
train 756840
val 103635
test 214284
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
























































































































































































1000it [06:09,  2.71it/s]
	iters: 1000, epoch: 1 | loss: 0.7209528
























































































1477it [09:05,  2.70it/s]
1478it [09:05,  2.71it/s]
















202it [00:33,  6.07it/s]
Epoch: 1, Steps: 1478 | Train Loss: 0.5421719 Vali Loss: 0.5461939
lr_adjust = {1: 0.0001}
Updating learning rate to 0.0001
202it [00:33,  6.03it/s]
























































































































































































1002it [06:09,  2.71it/s]
	iters: 1000, epoch: 2 | loss: 0.4685694























































































1478it [09:05,  2.71it/s]
0it [00:00, ?it/s]

















202it [00:33,  6.05it/s]
Epoch: 2, Steps: 1478 | Train Loss: 0.5256005 Vali Loss: 0.5384626
lr_adjust = {2: 0.0001}
Updating learning rate to 0.0001
Validation loss decreased (0.546194 --> 0.538463).  Saving model ...
























































































































































































1002it [06:09,  2.72it/s]
	iters: 1000, epoch: 3 | loss: 0.4000067























































































1478it [09:05,  2.71it/s]
1it [00:00,  5.82it/s]

















202it [00:33,  6.05it/s]
Epoch: 3, Steps: 1478 | Train Loss: 0.5171925 Vali Loss: 0.5323047
lr_adjust = {3: 0.0001}
Updating learning rate to 0.0001
Validation loss decreased (0.538463 --> 0.532305).  Saving model ...























































































































































































998it [06:08,  2.71it/s]
	iters: 1000, epoch: 4 | loss: 0.3617823
























































































1474it [09:04,  2.71it/s]
1478it [09:05,  2.71it/s]
















195it [00:32,  6.06it/s]
Epoch: 4, Steps: 1478 | Train Loss: 0.5099379 Vali Loss: 0.5315573
lr_adjust = {4: 9e-05}
Updating learning rate to 9e-05

202it [00:33,  6.05it/s]























































































































































































998it [06:08,  2.71it/s]
	iters: 1000, epoch: 5 | loss: 0.4188100
























































































1475it [09:04,  2.71it/s]
1478it [09:05,  2.71it/s]
















199it [00:32,  6.06it/s]
Epoch: 5, Steps: 1478 | Train Loss: 0.5037115 Vali Loss: 0.5329115
lr_adjust = {5: 8.1e-05}
Updating learning rate to 8.1e-05
202it [00:33,  6.04it/s]























































































































































































995it [06:07,  2.71it/s]
	iters: 1000, epoch: 6 | loss: 0.3917640

























































































1478it [09:05,  2.71it/s]
Epoch: 6 cost time: 545.9150564670563
















202it [00:33,  6.04it/s]
Epoch: 6, Steps: 1478 | Train Loss: 0.4988657 Vali Loss: 0.5362720
lr_adjust = {6: 7.290000000000001e-05}
Updating learning rate to 7.290000000000001e-05
EarlyStopping counter: 2 out of 3























































































































































































998it [06:08,  2.70it/s]
	iters: 1000, epoch: 7 | loss: 0.5371506

























































































1477it [38:12,  2.71it/s]
1478it [38:13,  1.55s/it]
















199it [00:32,  6.08it/s]
Epoch: 7, Steps: 1478 | Train Loss: 0.4944033 Vali Loss: 0.5343941
lr_adjust = {7: 6.561e-05}
Updating learning rate to 6.561e-05
EarlyStopping counter: 3 out of 3
Early stopping
202it [00:33,  6.06it/s]
































409it [01:05,  6.34it/s]
test shape: (418, 512, 336, 1) (418, 512, 336, 1)

418it [01:06,  6.28it/s]
mae:0.2855, mse:0.2481, rmse:0.4981, r2:0.5725
mse_mean = 0.2517, mse_std = 0.0041
r2_mean = 0.5663, mae_std = 0.0071