train 748776
val 95571
test 206220
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)


























































































































































































1000it [06:13,  2.68it/s]
	iters: 1000, epoch: 1 | loss: 0.4909866






















































































1462it [09:06,  2.68it/s]
Epoch: 1 cost time: 546.6972951889038














178it [00:29,  6.01it/s]
Epoch: 1, Steps: 1462 | Train Loss: 0.5996349 Vali Loss: 0.6422023
lr_adjust = {1: 0.0001}
Updating learning rate to 0.0001

186it [00:31,  5.97it/s]

























































































































































































998it [06:12,  2.68it/s]
	iters: 1000, epoch: 2 | loss: 0.6462466






















































































1460it [09:04,  2.68it/s]
1462it [09:05,  2.68it/s]















184it [00:30,  6.00it/s]
Epoch: 2, Steps: 1462 | Train Loss: 0.5847118 Vali Loss: 0.6284525
lr_adjust = {2: 0.0001}
Updating learning rate to 0.0001
186it [00:31,  5.97it/s]


























































































































































































999it [06:12,  2.68it/s]
	iters: 1000, epoch: 3 | loss: 0.5145264






















































































1460it [09:04,  2.69it/s]
1462it [09:05,  2.68it/s]















186it [00:31,  5.99it/s]
Epoch: 3, Steps: 1462 | Train Loss: 0.5766189 Vali Loss: 0.6353968
lr_adjust = {3: 0.0001}
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 3

























































































































































































998it [06:12,  2.68it/s]
	iters: 1000, epoch: 4 | loss: 0.4595831






















































































1460it [09:04,  2.69it/s]
1462it [09:05,  2.68it/s]















184it [00:30,  6.02it/s]
Epoch: 4, Steps: 1462 | Train Loss: 0.5703016 Vali Loss: 0.6332567
lr_adjust = {4: 9e-05}
Updating learning rate to 9e-05
186it [00:31,  5.98it/s]

























































































































































































997it [06:11,  2.68it/s]
	iters: 1000, epoch: 5 | loss: 0.4371649






















































































1459it [09:04,  2.68it/s]
1462it [09:05,  2.68it/s]















183it [00:30,  5.98it/s]
Epoch: 5, Steps: 1462 | Train Loss: 0.5642616 Vali Loss: 0.6336733
lr_adjust = {5: 8.1e-05}
Updating learning rate to 8.1e-05
EarlyStopping counter: 3 out of 3
Early stopping
186it [00:31,  5.96it/s]
































402it [01:03,  6.32it/s]
test shape: (402, 512, 720, 1) (402, 512, 720, 1)
test shape: (205824, 720, 1) (205824, 720, 1)
mae:0.3331, mse:0.3171, rmse:0.5631, r2:0.4518
train 748776
val 95571
test 206220
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)


























































































































































































999it [06:13,  2.68it/s]
	iters: 1000, epoch: 1 | loss: 0.4234923






















































































1460it [09:05,  2.68it/s]
1462it [09:06,  2.68it/s]















185it [00:30,  5.99it/s]
Epoch: 1, Steps: 1462 | Train Loss: 0.6140007 Vali Loss: 0.6334370
lr_adjust = {1: 0.0001}
Updating learning rate to 0.0001
186it [00:31,  5.98it/s]


























































































































































































1000it [06:13,  2.68it/s]
	iters: 1000, epoch: 2 | loss: 0.5954207






















































































1461it [09:05,  2.68it/s]
1462it [09:05,  2.68it/s]















186it [00:31,  5.97it/s]
Epoch: 2, Steps: 1462 | Train Loss: 0.5937306 Vali Loss: 0.6382173
lr_adjust = {2: 0.0001}
Updating learning rate to 0.0001
EarlyStopping counter: 1 out of 3


























































































































































































1001it [06:13,  2.67it/s]
	iters: 1000, epoch: 3 | loss: 0.6411849






















































































1462it [09:06,  2.68it/s]
Epoch: 3 cost time: 546.7127418518066














186it [00:31,  5.96it/s]
0it [00:00, ?it/s]
Epoch: 3, Steps: 1462 | Train Loss: 0.5868964 Vali Loss: 0.6360902
lr_adjust = {3: 0.0001}
Updating learning rate to 0.0001


























































































































































































996it [06:11,  2.68it/s]
	iters: 1000, epoch: 4 | loss: 0.4839326






















































































1462it [09:05,  2.68it/s]
1it [00:00,  5.62it/s]















181it [00:30,  5.98it/s]
Epoch: 4, Steps: 1462 | Train Loss: 0.5805185 Vali Loss: 0.6298301
lr_adjust = {4: 9e-05}
Updating learning rate to 9e-05

186it [00:31,  5.96it/s]

























































































































































































998it [06:12,  2.67it/s]
	iters: 1000, epoch: 5 | loss: 0.6172183



































































