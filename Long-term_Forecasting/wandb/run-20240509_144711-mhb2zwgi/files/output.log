['date', 'atq', 'ni', 'dv', 'acc', 'invest', 'mc', 'bm', 'dinvt', 'dar', 'capx', 'gm', 'sga', 'prc', 'ret', 'vol', 'shrout', 'medest', 'meanest', 'value']
train 337
['date', 'atq', 'ni', 'dv', 'acc', 'invest', 'mc', 'bm', 'dinvt', 'dar', 'capx', 'gm', 'sga', 'prc', 'ret', 'vol', 'shrout', 'medest', 'meanest', 'value']
val 43
['date', 'atq', 'ni', 'dv', 'acc', 'invest', 'mc', 'bm', 'dinvt', 'dar', 'capx', 'gm', 'sga', 'prc', 'ret', 'vol', 'shrout', 'medest', 'meanest', 'value']
test 93
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
1it [00:01,  1.10s/it]
0it [00:00, ?it/s]
outputs torch.Size([337, 12, 18])
B 337
L 18
M 18
Epoch: 1 cost time: 1.3697829246520996
outputs torch.Size([43, 12, 18])
B 43
L 18
M 18
total vali observation: torch.Size([172])
missing_idx_len: 43
pred tensor([0.2149, 0.2442, 0.2277, 0.1946, 0.2137, 0.2755, 0.2040, 0.2680, 0.2552,
        0.1870, 0.2421, 0.2452, 0.2095, 0.2597, 0.1884, 0.2532, 0.1936, 0.2094,
        0.2765, 0.2266, 0.2370, 0.2617, 0.2311, 0.1734, 0.2561, 0.2091, 0.2454,
        0.2383, 0.2226, 0.2318, 0.2284, 0.2121, 0.2723, 0.0984, 0.3114, 0.1324,
        0.2193, 0.2273, 0.2552, 0.2257, 0.2184, 0.2046, 0.2457, 0.1908, 0.2656,
        0.1900, 0.2361, 0.2823, 0.2348, 0.1007, 0.4236, 0.1611, 0.2501, 0.1855,
        0.2507, 0.1616, 0.2204, 0.2190, 0.2190, 0.2512, 0.2261, 0.2367, 0.2312,
        0.2256, 0.2828, 0.1798, 0.2817, 0.1594, 0.2477, 0.1966, 0.2340, 0.2392,
        0.2046, 0.2390, 0.2463, 0.2031, 0.2026, 0.2127, 0.2557, 0.2385, 0.2116,
        0.2193, 0.2270, 0.2321, 0.1738, 0.2305, 0.1999, 0.2164, 0.2032, 0.2573,
        0.2056, 0.2720, 0.2613, 0.2491, 0.2713, 0.2085, 0.2144, 0.2401, 0.2033,
        0.2406, 0.1953, 0.2744, 0.2805, 0.0979, 0.2255, 0.1758, 0.2102, 0.2328,
        0.1851, 0.2339, 0.1949, 0.2160, 0.2412, 0.2207, 0.2077, 0.2492, 0.1730,
        0.2965, 0.2021, 0.1416, 0.1980, 0.2293, 0.2108, 0.2325, 0.1678, 0.2018,
        0.2671, 0.2178, 0.1827, 0.2170, 0.1768, 0.2470, 0.2416, 0.2118, 0.2304,
        0.2240, 0.2327, 0.2132, 0.2206, 0.2096, 0.3409, 0.2966, 0.3078, 0.2344,
        0.2450, 0.1621, 0.2655, 0.2000, 0.1690, 0.2502, 0.2154, 0.2238, 0.2400,
        0.2640, 0.1875, 0.2590, 0.2219, 0.2232, 0.2168, 0.2082, 0.1955, 0.2548,
        0.1842, 0.2431, 0.2160, 0.2064, 0.2162, 0.2175, 0.3173, 0.2265, 0.1934,
        0.0620])
true tensor([-0.7600, -0.6300, -0.5400, -0.5700, -0.1300, -0.1600, -0.3800, -0.2800,
        -0.2100, -0.1300, -0.1600, -0.3800, -0.2000, -0.2100, -0.1300, -0.1600,
        -0.3300, -0.2000, -0.2100, -0.1300, -0.2100, -0.2300, -0.1900, -0.7600,
        -0.5400, -0.5700, -0.5400, -0.4800, -0.6300, -0.5400, -0.5700, -0.5400,
        -0.3300, -0.2000, -0.2100, -0.1300, -0.3800, -0.2800, -0.2100, -0.2300,
        -0.2800, -0.2100, -0.2300, -0.1900, -0.5400, -0.5700, -0.5400, -0.4800,
        -0.1600, -0.3800, -0.2800, -0.2100, -0.7600, -0.6300, -0.5400, -0.5700,
        -0.1900, -0.7600, -0.6300, -0.5400, -0.2100, -0.1300, -0.1600, -0.3800,
        -0.5400, -0.5700, -0.5400, -0.4800, -0.2300, -0.1900, -0.7600, -0.6300,
        -0.3800, -0.2800, -0.2100, -0.2300, -0.5700, -0.5400, -0.4800, -0.4900,
        -0.3800, -0.2800, -0.2100, -0.2300, -0.6300, -0.5400, -0.5700, -0.5400,
        -0.1900, -0.7600, -0.6300, -0.5400, -0.1300, -0.1600, -0.3800, -0.2800,
        -0.6300, -0.5400, -0.5700, -0.5400, -0.3300, -0.2000, -0.2100, -0.1300,
        -0.2800, -0.2100, -0.2300, -0.1900, -0.1900, -0.7600, -0.6300, -0.5400,
        -0.1600, -0.3800, -0.2800, -0.2100, -0.2000, -0.2100, -0.1300, -0.1600,
        -0.5700, -0.5400, -0.4800, -0.4900, -0.2800, -0.2100, -0.2300, -0.1900,
        -0.2100, -0.2300, -0.1900, -0.7600, -0.7600, -0.6300, -0.5400, -0.5700,
        -0.2100, -0.2300, -0.1900, -0.7600, -0.2300, -0.1900, -0.7600, -0.6300,
        -0.5400, -0.4800, -0.4900, -0.4700, -0.5700, -0.5400, -0.4800, -0.4900,
        -0.2300, -0.1900, -0.7600, -0.6300, -0.2100, -0.1300, -0.1600, -0.3800,
        -0.1300, -0.1600, -0.3800, -0.2800, -0.2000, -0.2100, -0.1300, -0.1600,
        -0.1600, -0.3800, -0.2800, -0.2100])
Epoch: 1, Steps: 1 | Train Loss: 11.1899252 Vali Loss: 0.3971287
lr = 0.0000975531
1it [00:00, 12.71it/s]
1it [00:00,  6.41it/s]
1it [00:00, 11.52it/s]
outputs torch.Size([337, 12, 18])
B 337
L 18
M 18
Epoch: 2 cost time: 0.4885070323944092
outputs torch.Size([43, 12, 18])
B 43
L 18
M 18
total vali observation: torch.Size([172])
missing_idx_len: 43
pred tensor([0.2438, 0.2920, 0.1503, 0.2412, 0.2169, 0.1975, 0.2350, 0.1794, 0.2318,
        0.1982, 0.2165, 0.2243, 0.2190, 0.1830, 0.1692, 0.1908, 0.2373, 0.2317,
        0.2184, 0.2223, 0.2214, 0.2233, 0.2278, 0.2187, 0.2420, 0.2581, 0.1833,
        0.2475, 0.1786, 0.2594, 0.1721, 0.2077, 0.2298, 0.2053, 0.2055, 0.2471,
        0.2064, 0.2349, 0.2187, 0.2442, 0.2614, 0.1311, 0.2139, 0.1377, 0.1820,
        0.2236, 0.2081, 0.2536, 0.2073, 0.2796, 0.2001, 0.2407, 0.1928, 0.2484,
        0.2359, 0.2169, 0.2338, 0.2392, 0.2158, 0.2114, 0.2027, 0.2273, 0.2710,
        0.1540, 0.2170, 0.2432, 0.2272, 0.1909, 0.1940, 0.2494, 0.1906, 0.2577,
        0.2500, 0.2055, 0.2526, 0.2438, 0.2454, 0.2155, 0.1947, 0.2471, 0.1811,
        0.1706, 0.1949, 0.3263, 0.2305, 0.1970, 0.2156, 0.1911, 0.2503, 0.2023,
        0.2221, 0.2244, 0.2469, 0.2180, 0.2334, 0.2395, 0.2433, 0.2228, 0.1750,
        0.3085, 0.2153, 0.2040, 0.1723, 0.1912, 0.2117, 0.2419, 0.1946, 0.2377,
        0.2203, 0.2451, 0.2016, 0.2285, 0.2023, 0.2382, 0.2307, 0.2202, 0.2933,
        0.2914, 0.2533, 0.1761, 0.1986, 0.2180, 0.2321, 0.2281, 0.2712, 0.2216,
        0.2317, 0.1995, 0.2232, 0.2142, 0.2175, 0.1958, 0.1640, 0.1463, 0.2547,
        0.1040, 0.2158, 0.2566, 0.2232, 0.2078, 0.1819, 0.2660, 0.1968, 0.1971,
        0.2002, 0.2371, 0.2192, 0.2355, 0.2291, 0.2626, 0.2143, 0.2129, 0.1729,
        0.2446, 0.1686, 0.1829, 0.2855, 0.2029, 0.2703, 0.2367, 0.2031, 0.2253,
        0.2372, 0.2330, 0.1677, 0.2216, 0.1862, 0.2417, 0.2623, 0.3831, 0.0394,
        0.3756])
true tensor([-0.2300, -0.1900, -0.7600, -0.6300, -0.7600, -0.6300, -0.5400, -0.5700,
        -0.2000, -0.2100, -0.1300, -0.1600, -0.2100, -0.2300, -0.1900, -0.7600,
        -0.2100, -0.1300, -0.1600, -0.3800, -0.5700, -0.5400, -0.4800, -0.4900,
        -0.1900, -0.7600, -0.6300, -0.5400, -0.6300, -0.5400, -0.5700, -0.5400,
        -0.2300, -0.1900, -0.7600, -0.6300, -0.1600, -0.3800, -0.2800, -0.2100,
        -0.3300, -0.2000, -0.2100, -0.1300, -0.5700, -0.5400, -0.4800, -0.4900,
        -0.1300, -0.1600, -0.3800, -0.2800, -0.1300, -0.1600, -0.3800, -0.2800,
        -0.2100, -0.1300, -0.1600, -0.3800, -0.3800, -0.2800, -0.2100, -0.2300,
        -0.6300, -0.5400, -0.5700, -0.5400, -0.2000, -0.2100, -0.1300, -0.1600,
        -0.5400, -0.5700, -0.5400, -0.4800, -0.5400, -0.4800, -0.4900, -0.4700,
        -0.3300, -0.2000, -0.2100, -0.1300, -0.5400, -0.5700, -0.5400, -0.4800,
        -0.2800, -0.2100, -0.2300, -0.1900, -0.5400, -0.5700, -0.5400, -0.4800,
        -0.1900, -0.7600, -0.6300, -0.5400, -0.2800, -0.2100, -0.2300, -0.1900,
        -0.5700, -0.5400, -0.4800, -0.4900, -0.6300, -0.5400, -0.5700, -0.5400,
        -0.2800, -0.2100, -0.2300, -0.1900, -0.2300, -0.1900, -0.7600, -0.6300,
        -0.3800, -0.2800, -0.2100, -0.2300, -0.7600, -0.6300, -0.5400, -0.5700,
        -0.1900, -0.7600, -0.6300, -0.5400, -0.2000, -0.2100, -0.1300, -0.1600,
        -0.7600, -0.6300, -0.5400, -0.5700, -0.1600, -0.3800, -0.2800, -0.2100,
        -0.1300, -0.1600, -0.3800, -0.2800, -0.2100, -0.2300, -0.1900, -0.7600,
        -0.3300, -0.2000, -0.2100, -0.1300, -0.2100, -0.1300, -0.1600, -0.3800,
        -0.3800, -0.2800, -0.2100, -0.2300, -0.2100, -0.2300, -0.1900, -0.7600,
        -0.1600, -0.3800, -0.2800, -0.2100])
Epoch: 2, Steps: 1 | Train Loss: 11.2180729 Vali Loss: 0.3925166
lr = 0.0000904518
Validation loss decreased (0.397129 --> 0.392517).  Saving model ...
1it [00:00,  5.73it/s]
1it [00:00, 11.74it/s]
1it [00:00,  6.11it/s]
outputs torch.Size([337, 12, 18])
B 337
L 18
M 18
Epoch: 3 cost time: 0.5212597846984863
outputs torch.Size([43, 12, 18])
B 43
L 18
M 18
total vali observation: torch.Size([172])
missing_idx_len: 43
pred tensor([0.2299, 0.2372, 0.2377, 0.1981, 0.2297, 0.2600, 0.2138, 0.2400, 0.2231,
        0.2759, 0.2366, 0.2215, 0.2186, 0.2431, 0.2153, 0.2442, 0.2213, 0.2158,
        0.1985, 0.2100, 0.2425, 0.1976, 0.1943, 0.2002, 0.1386, 0.2600, 0.2610,
        0.1789, 0.1859, 0.1384, 0.1705, 0.2002, 0.2205, 0.2453, 0.1862, 0.2478,
        0.1358, 0.1505, 0.2443, 0.2174, 0.1964, 0.2830, 0.2150, 0.2235, 0.1932,
        0.2052, 0.1851, 0.2324, 0.4263, 0.2794, 0.1851, 0.2551, 0.2482, 0.2182,
        0.2187, 0.2425, 0.2380, 0.2312, 0.2514, 0.2333, 0.2162, 0.2201, 0.2336,
        0.2226, 0.2437, 0.2364, 0.1992, 0.2493, 0.2166, 0.2022, 0.2283, 0.1807,
        0.2271, 0.2856, 0.2003, 0.2402, 0.2148, 0.1943, 0.2307, 0.2123, 0.2033,
        0.1912, 0.2297, 0.2263, 0.2104, 0.2288, 0.2378, 0.2430, 0.2198, 0.2430,
        0.2317, 0.2288, 0.2412, 0.2044, 0.2244, 0.2761, 0.2172, 0.2349, 0.2265,
        0.2423, 0.2479, 0.1630, 0.2548, 0.1941, 0.2037, 0.2370, 0.1864, 0.2428,
        0.2086, 0.2504, 0.2180, 0.2063, 0.2184, 0.1919, 0.1957, 0.2243, 0.1788,
        0.2531, 0.2275, 0.2285, 0.3025, 0.2415, 0.2654, 0.2773, 0.1988, 0.2355,
        0.2047, 0.2391, 0.2140, 0.2334, 0.1972, 0.2250, 0.2423, 0.2042, 0.2009,
        0.2242, 0.2152, 0.2412, 0.1991, 0.2555, 0.2101, 0.2046, 0.2291, 0.2143,
        0.2416, 0.2088, 0.2481, 0.2107, 0.2178, 0.2821, 0.1854, 0.2743, 0.1992,
        0.2408, 0.2158, 0.2427, 0.2350, 0.1557, 0.1043, 0.1398, 0.2417, 0.2091,
        0.2219, 0.2612, 0.2362, 0.2648, 0.1416, 0.2942, 0.1687, 0.2089, 0.2474,
        0.0903])
true tensor([-0.2100, -0.2300, -0.1900, -0.7600, -0.6300, -0.5400, -0.5700, -0.5400,
        -0.7600, -0.6300, -0.5400, -0.5700, -0.1900, -0.7600, -0.6300, -0.5400,
        -0.2100, -0.1300, -0.1600, -0.3800, -0.1900, -0.7600, -0.6300, -0.5400,
        -0.1600, -0.3800, -0.2800, -0.2100, -0.2000, -0.2100, -0.1300, -0.1600,
        -0.1900, -0.7600, -0.6300, -0.5400, -0.3300, -0.2000, -0.2100, -0.1300,
        -0.1300, -0.1600, -0.3800, -0.2800, -0.3800, -0.2800, -0.2100, -0.2300,
        -0.1600, -0.3800, -0.2800, -0.2100, -0.2300, -0.1900, -0.7600, -0.6300,
        -0.5400, -0.5700, -0.5400, -0.4800, -0.2100, -0.1300, -0.1600, -0.3800,
        -0.5400, -0.4800, -0.4900, -0.4700, -0.5700, -0.5400, -0.4800, -0.4900,
        -0.5400, -0.5700, -0.5400, -0.4800, -0.3800, -0.2800, -0.2100, -0.2300,
        -0.7600, -0.6300, -0.5400, -0.5700, -0.2800, -0.2100, -0.2300, -0.1900,
        -0.6300, -0.5400, -0.5700, -0.5400, -0.2100, -0.1300, -0.1600, -0.3800,
        -0.1600, -0.3800, -0.2800, -0.2100, -0.2800, -0.2100, -0.2300, -0.1900,
        -0.6300, -0.5400, -0.5700, -0.5400, -0.7600, -0.6300, -0.5400, -0.5700,
        -0.2800, -0.2100, -0.2300, -0.1900, -0.5700, -0.5400, -0.4800, -0.4900,
        -0.2300, -0.1900, -0.7600, -0.6300, -0.5700, -0.5400, -0.4800, -0.4900,
        -0.3800, -0.2800, -0.2100, -0.2300, -0.2000, -0.2100, -0.1300, -0.1600,
        -0.1300, -0.1600, -0.3800, -0.2800, -0.2100, -0.2300, -0.1900, -0.7600,
        -0.5400, -0.5700, -0.5400, -0.4800, -0.1300, -0.1600, -0.3800, -0.2800,
        -0.2000, -0.2100, -0.1300, -0.1600, -0.3300, -0.2000, -0.2100, -0.1300,
        -0.2100, -0.2300, -0.1900, -0.7600, -0.2300, -0.1900, -0.7600, -0.6300,
        -0.3300, -0.2000, -0.2100, -0.1300])
Epoch: 3, Steps: 1 | Train Loss: 11.2421188 Vali Loss: 0.3952209
lr = 0.0000793913
EarlyStopping counter: 1 out of 3
outputs torch.Size([337, 12, 18])
B 337
L 18
M 18
Epoch: 4 cost time: 0.502995491027832
outputs torch.Size([43, 12, 18])
B 43
L 18
M 18
total vali observation: torch.Size([172])
missing_idx_len: 43
pred tensor([0.1755, 0.2412, 0.2272, 0.2254, 0.3008, 0.1749, 0.2550, 0.3171, 0.1989,
        0.2218, 0.2197, 0.2135, 0.2204, 0.2404, 0.2376, 0.2050, 0.2267, 0.2244,
        0.2277, 0.2306, 0.2159, 0.2705, 0.2332, 0.2496, 0.2428, 0.2000, 0.2682,
        0.2287, 0.2376, 0.1626, 0.2892, 0.1903, 0.2247, 0.2661, 0.1921, 0.2188,
        0.2539, 0.2128, 0.2188, 0.1936, 0.2063, 0.2623, 0.2435, 0.1826, 0.2647,
        0.2156, 0.2221, 0.2519, 0.2154, 0.2752, 0.2264, 0.2172, 0.2014, 0.2224,
        0.2694, 0.1933, 0.2324, 0.2475, 0.2132, 0.2329, 0.2173, 0.1923, 0.2116,
        0.1996, 0.2216, 0.2047, 0.2342, 0.2221, 0.2163, 0.2106, 0.2110, 0.1686,
        0.2252, 0.2807, 0.1820, 0.2413, 0.1328, 0.2377, 0.1946, 0.2530, 0.1830,
        0.2183, 0.1679, 0.2635, 0.2649, 0.2302, 0.1654, 0.2772, 0.2111, 0.1933,
        0.2246, 0.2369, 0.2390, 0.2504, 0.2497, 0.3189, 0.2767, 0.1624, 0.2452,
        0.2347, 0.1614, 0.2781, 0.2877, 0.2092, 0.1806, 0.1904, 0.2327, 0.2695,
        0.2269, 0.2382, 0.2541, 0.1653, 0.2800, 0.1697, 0.2331, 0.2998, 0.2198,
        0.2382, 0.2366, 0.1885, 0.2577, 0.2333, 0.2400, 0.2151, 0.2381, 0.2225,
        0.2103, 0.2197, 0.2196, 0.2382, 0.1730, 0.2225, 0.1820, 0.2999, 0.1896,
        0.1367, 0.2138, 0.2383, 0.1988, 0.2175, 0.2716, 0.1948, 0.2377, 0.2348,
        0.2619, 0.2058, 0.2130, 0.2096, 0.2409, 0.2521, 0.1989, 0.2300, 0.2329,
        0.2271, 0.1949, 0.2474, 0.2487, 0.2440, 0.2541, 0.2246, 0.2333, 0.2209,
        0.2495, 0.2678, 0.2527, 0.2622, 0.3440, 0.2320, 0.1774, 0.2200, 0.2030,
        0.2259])
true tensor([-0.5700, -0.5400, -0.4800, -0.4900, -0.2300, -0.1900, -0.7600, -0.6300,
        -0.2100, -0.2300, -0.1900, -0.7600, -0.3800, -0.2800, -0.2100, -0.2300,
        -0.5400, -0.4800, -0.4900, -0.4700, -0.1300, -0.1600, -0.3800, -0.2800,
        -0.3800, -0.2800, -0.2100, -0.2300, -0.3300, -0.2000, -0.2100, -0.1300,
        -0.1900, -0.7600, -0.6300, -0.5400, -0.7600, -0.6300, -0.5400, -0.5700,
        -0.2000, -0.2100, -0.1300, -0.1600, -0.2300, -0.1900, -0.7600, -0.6300,
        -0.1600, -0.3800, -0.2800, -0.2100, -0.2800, -0.2100, -0.2300, -0.1900,
        -0.6300, -0.5400, -0.5700, -0.5400, -0.2800, -0.2100, -0.2300, -0.1900,
        -0.2800, -0.2100, -0.2300, -0.1900, -0.3300, -0.2000, -0.2100, -0.1300,
        -0.2300, -0.1900, -0.7600, -0.6300, -0.3800, -0.2800, -0.2100, -0.2300,
        -0.2100, -0.2300, -0.1900, -0.7600, -0.1900, -0.7600, -0.6300, -0.5400,
        -0.5700, -0.5400, -0.4800, -0.4900, -0.7600, -0.6300, -0.5400, -0.5700,
        -0.7600, -0.6300, -0.5400, -0.5700, -0.1300, -0.1600, -0.3800, -0.2800,
        -0.3300, -0.2000, -0.2100, -0.1300, -0.6300, -0.5400, -0.5700, -0.5400,
        -0.2100, -0.1300, -0.1600, -0.3800, -0.2100, -0.2300, -0.1900, -0.7600,
        -0.2100, -0.1300, -0.1600, -0.3800, -0.2000, -0.2100, -0.1300, -0.1600,
        -0.2100, -0.1300, -0.1600, -0.3800, -0.1600, -0.3800, -0.2800, -0.2100,
        -0.1300, -0.1600, -0.3800, -0.2800, -0.5400, -0.5700, -0.5400, -0.4800,
        -0.5400, -0.5700, -0.5400, -0.4800, -0.2000, -0.2100, -0.1300, -0.1600,
        -0.1900, -0.7600, -0.6300, -0.5400, -0.5400, -0.5700, -0.5400, -0.4800,
        -0.5700, -0.5400, -0.4800, -0.4900, -0.1600, -0.3800, -0.2800, -0.2100,
        -0.6300, -0.5400, -0.5700, -0.5400])
Epoch: 4, Steps: 1 | Train Loss: 11.2592735 Vali Loss: 0.4012032
lr = 0.0000654543
EarlyStopping counter: 2 out of 3
1it [00:00,  9.76it/s]
1it [00:00,  6.41it/s]
1it [00:00, 11.58it/s]
1it [00:00, 10.99it/s]
outputs torch.Size([337, 12, 18])
B 337
L 18
M 18
Epoch: 5 cost time: 0.49799132347106934
outputs torch.Size([43, 12, 18])
B 43
L 18
M 18
total vali observation: torch.Size([172])
missing_idx_len: 43
pred tensor([0.2263, 0.1623, 0.1870, 0.2015, 0.2116, 0.2171, 0.2462, 0.2242, 0.1912,
        0.2947, 0.2190, 0.2240, 0.2174, 0.2350, 0.2295, 0.2145, 0.2015, 0.2358,
        0.2390, 0.2210, 0.2515, 0.2211, 0.2412, 0.2461, 0.2481, 0.2104, 0.2490,
        0.1680, 0.2078, 0.2252, 0.2412, 0.2256, 0.2600, 0.1830, 0.2556, 0.2372,
        0.2071, 0.1564, 0.2546, 0.2556, 0.2133, 0.2233, 0.2415, 0.2076, 0.2106,
        0.1991, 0.2310, 0.2255, 0.2527, 0.1869, 0.2443, 0.2513, 0.2500, 0.1952,
        0.2230, 0.2050, 0.2208, 0.2135, 0.1978, 0.2261, 0.3123, 0.2559, 0.2432,
        0.2943, 0.2665, 0.2346, 0.2307, 0.2113, 0.2306, 0.2318, 0.1877, 0.2613,
        0.1949, 0.2415, 0.1682, 0.2628, 0.2017, 0.2212, 0.2109, 0.2258, 0.1986,
        0.1838, 0.2220, 0.2074, 0.1984, 0.2859, 0.1904, 0.2566, 0.2264, 0.2397,
        0.2499, 0.2036, 0.3403, 0.1686, 0.2159, 0.2563, 0.2587, 0.2552, 0.2312,
        0.2777, 0.1959, 0.3100, 0.2171, 0.2164, 0.2570, 0.2252, 0.2089, 0.2424,
        0.1963, 0.1917, 0.2654, 0.2135, 0.1918, 0.2267, 0.2447, 0.2474, 0.2165,
        0.2351, 0.2513, 0.1953, 0.2862, 0.1120, 0.2214, 0.2139, 0.1038, 0.2880,
        0.3145, 0.0765, 0.2239, 0.2771, 0.2201, 0.2502, 0.2150, 0.1833, 0.1953,
        0.1910, 0.2425, 0.2147, 0.2396, 0.2402, 0.2110, 0.2588, 0.2116, 0.2306,
        0.2140, 0.2344, 0.2329, 0.2088, 0.2119, 0.2424, 0.2248, 0.2277, 0.2160,
        0.2216, 0.1915, 0.2109, 0.2217, 0.2434, 0.1720, 0.2637, 0.1637, 0.2449,
        0.2065, 0.2083, 0.2074, 0.2182, 0.2159, 0.2495, 0.1724, 0.2026, 0.2208,
        0.1504])
true tensor([-0.2800, -0.2100, -0.2300, -0.1900, -0.5700, -0.5400, -0.4800, -0.4900,
        -0.1300, -0.1600, -0.3800, -0.2800, -0.7600, -0.6300, -0.5400, -0.5700,
        -0.5400, -0.4800, -0.4900, -0.4700, -0.2100, -0.1300, -0.1600, -0.3800,
        -0.5400, -0.5700, -0.5400, -0.4800, -0.2800, -0.2100, -0.2300, -0.1900,
        -0.5400, -0.5700, -0.5400, -0.4800, -0.3300, -0.2000, -0.2100, -0.1300,
        -0.7600, -0.6300, -0.5400, -0.5700, -0.2000, -0.2100, -0.1300, -0.1600,
        -0.2100, -0.1300, -0.1600, -0.3800, -0.2800, -0.2100, -0.2300, -0.1900,
        -0.6300, -0.5400, -0.5700, -0.5400, -0.3800, -0.2800, -0.2100, -0.2300,
        -0.1600, -0.3800, -0.2800, -0.2100, -0.1900, -0.7600, -0.6300, -0.5400,
        -0.2100, -0.2300, -0.1900, -0.7600, -0.3800, -0.2800, -0.2100, -0.2300,
        -0.3800, -0.2800, -0.2100, -0.2300, -0.1300, -0.1600, -0.3800, -0.2800,
        -0.1300, -0.1600, -0.3800, -0.2800, -0.7600, -0.6300, -0.5400, -0.5700,
        -0.2300, -0.1900, -0.7600, -0.6300, -0.6300, -0.5400, -0.5700, -0.5400,
        -0.2300, -0.1900, -0.7600, -0.6300, -0.3300, -0.2000, -0.2100, -0.1300,
        -0.5700, -0.5400, -0.4800, -0.4900, -0.2100, -0.2300, -0.1900, -0.7600,
        -0.1600, -0.3800, -0.2800, -0.2100, -0.5700, -0.5400, -0.4800, -0.4900,
        -0.1900, -0.7600, -0.6300, -0.5400, -0.2000, -0.2100, -0.1300, -0.1600,
        -0.5400, -0.5700, -0.5400, -0.4800, -0.2000, -0.2100, -0.1300, -0.1600,
        -0.1900, -0.7600, -0.6300, -0.5400, -0.1600, -0.3800, -0.2800, -0.2100,
        -0.2100, -0.1300, -0.1600, -0.3800, -0.2300, -0.1900, -0.7600, -0.6300,
        -0.6300, -0.5400, -0.5700, -0.5400, -0.2100, -0.2300, -0.1900, -0.7600,
        -0.3300, -0.2000, -0.2100, -0.1300])
Epoch: 5, Steps: 1 | Train Loss: 11.2344980 Vali Loss: 0.3967920
lr = 0.0000500050
EarlyStopping counter: 3 out of 3
Early stopping
------------------------------------
outputs torch.Size([93, 12, 18])
B 93
L 18
M 18
test shape: (1, 270) (1, 270)
test shape: (1, 1, 270) (1, 1, 270)
mae:0.3970, mse:0.2269, rmse:0.4764, r2:-0.1188
mse_mean = 0.2269, mse_std = 0.0000
r2_mean = -0.1188, mae_std = 0.0000