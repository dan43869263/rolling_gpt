['date', 'atq_1', 'atq_2', 'atq_3', 'ni_1', 'ni_2', 'ni_3', 'dv_1', 'dv_2', 'dv_3', 'acc_1', 'acc_2', 'acc_3', 'invest_1', 'invest_2', 'invest_3', 'mc_1', 'mc_2', 'mc_3', 'bm_1', 'bm_2', 'bm_3', 'dinvt_1', 'dinvt_2', 'dinvt_3', 'dar_1', 'dar_2', 'dar_3', 'capx_1', 'capx_2', 'capx_3', 'gm_1', 'gm_2', 'gm_3', 'sga_1', 'sga_2', 'sga_3', 'prc_1', 'prc_2', 'prc_3', 'ret_1', 'ret_2', 'ret_3', 'vol_1', 'vol_2', 'vol_3', 'shrout_1', 'shrout_2', 'shrout_3', 'medest_1', 'medest_2', 'medest_3', 'meanest_1', 'meanest_2', 'meanest_3', 'value']
train 92
['date', 'atq_1', 'atq_2', 'atq_3', 'ni_1', 'ni_2', 'ni_3', 'dv_1', 'dv_2', 'dv_3', 'acc_1', 'acc_2', 'acc_3', 'invest_1', 'invest_2', 'invest_3', 'mc_1', 'mc_2', 'mc_3', 'bm_1', 'bm_2', 'bm_3', 'dinvt_1', 'dinvt_2', 'dinvt_3', 'dar_1', 'dar_2', 'dar_3', 'capx_1', 'capx_2', 'capx_3', 'gm_1', 'gm_2', 'gm_3', 'sga_1', 'sga_2', 'sga_3', 'prc_1', 'prc_2', 'prc_3', 'ret_1', 'ret_2', 'ret_3', 'vol_1', 'vol_2', 'vol_3', 'shrout_1', 'shrout_2', 'shrout_3', 'medest_1', 'medest_2', 'medest_3', 'meanest_1', 'meanest_2', 'meanest_3', 'value']
val 8
['date', 'atq_1', 'atq_2', 'atq_3', 'ni_1', 'ni_2', 'ni_3', 'dv_1', 'dv_2', 'dv_3', 'acc_1', 'acc_2', 'acc_3', 'invest_1', 'invest_2', 'invest_3', 'mc_1', 'mc_2', 'mc_3', 'bm_1', 'bm_2', 'bm_3', 'dinvt_1', 'dinvt_2', 'dinvt_3', 'dar_1', 'dar_2', 'dar_3', 'capx_1', 'capx_2', 'capx_3', 'gm_1', 'gm_2', 'gm_3', 'sga_1', 'sga_2', 'sga_3', 'prc_1', 'prc_2', 'prc_3', 'ret_1', 'ret_2', 'ret_3', 'vol_1', 'vol_2', 'vol_3', 'shrout_1', 'shrout_2', 'shrout_3', 'medest_1', 'medest_2', 'medest_3', 'meanest_1', 'meanest_2', 'meanest_3', 'value']
test 23
gpt2 = GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (1): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (2): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (3): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (4): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (5): GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
0it [00:00, ?it/s]
outputs torch.Size([92, 12, 54])
B 92
L 18
0it [00:04, ?it/s]
Traceback (most recent call last):
  File "/home/dan/NeurIPS2023-One-Fits-All/Long-term_Forecasting/main.py", line 182, in <module>
    outputs = model(batch_x, ii)
  File "/home/dan/anaconda3/envs/geo/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/dan/NeurIPS2023-One-Fits-All/Long-term_Forecasting/models/GPT4TS.py", line 81, in forward
    outputs = self.final_layer(outputs)  # Apply linear layer to reduce from M to 1
  File "/home/dan/anaconda3/envs/geo/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/dan/anaconda3/envs/geo/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1104x54 and 18x1)